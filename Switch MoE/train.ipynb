{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d65e34a",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Implementing Switch Mixture-of-Experts (Top-1 Routing)\n",
    "The Switch Transformer introduces a scalable version of the Mixture-of-Experts (MoE) architecture, where each token is routed to only one expert (top-1) instead of multiple experts.\n",
    "\n",
    "This makes it:\n",
    "- Efficient â†’ reduces computation and memory cost\n",
    "- Scalable â†’ enables training with billions of parameters\n",
    "- Balanced â†’ uses an auxiliary loss to encourage fair token distribution across experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048b6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79547f7d",
   "metadata": {},
   "source": [
    "### ðŸ’¡Defining SwitchMoE Class \n",
    "- Implements a lightweight and efficient Mixture-of-Experts (MoE) layer inspired by the Switch Transformer\n",
    "\n",
    "#### ðŸ’¡Key points of the implementation:\n",
    "\n",
    "- Router (Gating Network): A linear layer that routes each token to exactly one expert (top-1 selection)\n",
    "- Experts: Multiple feed-forward networks (FFNs), each acting as a specialist\n",
    "- Auxiliary Loss: A balancing loss that encourages tokens to be distributed fairly across experts, avoiding overloading a few experts\n",
    "\n",
    "#### ðŸ’¡Forward Pass:\n",
    "\n",
    "- Compute routing probabilities\n",
    "- Select top-1 expert for each token \n",
    "- Dispatch tokens to chosen experts.\n",
    "- Combine outputs and return along with auxiliary loss.\n",
    "\n",
    "This design replaces the standard feed-forward network in a Transformer block, making it more scalable and compute-efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Switch Transformer-syle MoE layer:\n",
    "        - Replaces the dence FFN in transformer block\n",
    "        - Uses a learned router to pick exactly 1 expert-per-token (top-1 routing)\n",
    "    \n",
    "    Shapes:\n",
    "        - x: (batch_size, seq_len, d_model) with batch_first=True\n",
    "        - returns: (batch_size, seq_len, d_model), aux_loss: scaler\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_experts: int, d_ff: int, dropout: float = 0.0, activation: str = 'relu'):\n",
    "        super().__init__()\n",
    "\n",
    "        # Router : linear -> logits\n",
    "        self.router= nn.Linear(d_model, num_experts)\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        # Experts : Each is a standard FFN (Linear -> Activation -> Dropout -> Linear -> Dropout)\n",
    "        act = {\"relu\": nn.ReLU(), \"gelu\": nn.GELU()}[activation]\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff),\n",
    "                act,\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_ff, d_model)\n",
    "            )\n",
    "\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def _compute_routing(self, x_flat):\n",
    "        \"\"\"\n",
    "        x_flat: (T, S, d_model) where T = batch * seq_len\n",
    "        Returns:\n",
    "            top1_idx: (T,) expert indices\n",
    "            probs: (T, E) softmax prob over expert\n",
    "            \"\"\"\n",
    "        logits = self.router(x_flat)                        # (T, E)\n",
    "        probs = F.softmax(logits, dim=-1)                   # (T, E)\n",
    "        top1_idx = torch.argmax(probs, dim=-1)              # (T,)\n",
    "        return top1_idx, probs \n",
    "\n",
    "    def _load_balancing_loss(self, probs, top1_idx):\n",
    "        \"\"\"\n",
    "        probs: (T, E) softmax over experts for each token\n",
    "        top1_idx: (T, ) chosen expert per token \n",
    "        \"\"\" \n",
    "        T, E= probs.shape                           \n",
    "        p_i = probs.mean(dim=0)                              # (E,)\n",
    "        one_hot = F.one_hot(top1_idx, num_classes=E).float() # (T, E)\n",
    "        f_i = one_hot.mean(dim=0)                            # (E,)\n",
    "        loss = E * torch.sum(p_i * f_i)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, S, d_model)\n",
    "        returns: \n",
    "            y: (B, S, d_model)\n",
    "            aux_loss: scaler tensor\n",
    "        \"\"\"\n",
    "        B, S, D= x.size()\n",
    "        T = B * S\n",
    "        x_flat = x.reshape(T, D)                             # (T, d_model)\n",
    "\n",
    "        # While routing, we need to stop grad only for argmax choice\n",
    "        with torch.no_grad:\n",
    "            top1_idx, probs = self._compute_routing(x_flat)\n",
    "        \n",
    "        # Compute the aux loss (with autograd on probs)\n",
    "        logits = self.router(x_flat)                        # (T, E)\n",
    "        probs_grad = F.softmax(logits, dim=-1)\n",
    "        aux_loss = self._load_balancing_Loss(probs_grad, top1_idx)\n",
    "\n",
    "        # Dispath tokens to their respective experts \n",
    "        y_flat = torch.zeros_like(x_flat)\n",
    "        for e in range(self.num_experts):\n",
    "            mask = (top1_idx == e)\n",
    "            if mask.any():\n",
    "                tokens_e = x_flat[mask]                     # (n_e, d_model)\n",
    "                out_e = self.experts[e](tokens_e)           # (n_e, d_model)\n",
    "                y_flat[mask] = out_e\n",
    "        \n",
    "        y = y_flat.view(B, S, D)\n",
    "        return y, aux_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
