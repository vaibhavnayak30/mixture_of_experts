{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629cfa46",
   "metadata": {},
   "source": [
    "# Implementing Tok-k Importance Routing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0038a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, activation, dropout):\n",
    "        super().__init__()\n",
    "        act = {\"relu\":nn.ReLU(), \"gelu\": nn.GELU()}[activation]\n",
    "        self.ffn = nn.Sequential(\n",
    "                nn.Linear(d_model, d_hidden),\n",
    "                act,\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_hidden, d_model)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a28e34",
   "metadata": {},
   "source": [
    "### MoE Layer With Importance Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebacf2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    def __init__(self, d_model: int, d_hidden: int, num_experts: int, activation: str, dropout: float, k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "\n",
    "        # Defining the router \n",
    "        self.router = nn.Linear(d_model, num_experts)\n",
    "\n",
    "        # Pool of experts \n",
    "        self.experts = nn.ModuleList(\n",
    "            [Expert(d_model, d_hidden, activation, dropout) for _ in range(self.num_experts)]\n",
    "        )\n",
    "\n",
    "    def _load_balancing_loss(self, probs: torch.tensor, top1_idx: torch.tensor):\n",
    "        _, E= probs.shape\n",
    "\n",
    "        # Calculate mean probability for each expert against all tokens\n",
    "        p_i = probs.mean(dim=0)                                        # (E,)\n",
    "\n",
    "        # Caculate fraction of tokens going to each expert \n",
    "        one_hot = F.one_hot(top1_idx, num_classes=self.num_experts).float()    # (T, E)\n",
    "        f_i = one_hot.mean(dim=0)                                      # (E,)\n",
    "\n",
    "        # Caculate loss\n",
    "        loss = E * torch.sum(f_i * p_i)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        x : (B, S, D)\n",
    "        \"\"\"\n",
    "        B, S, D = x.shape\n",
    "        T = B * S\n",
    "        x_flat = x.view(T, D)\n",
    "\n",
    "        # Routing\n",
    "        logits = self.router(x_flat)                                    # (T, E)\n",
    "        probs = F.softmax(logits, dim=-1)                               # (T, E)\n",
    "        topk_prob, topk_idx = torch.topk(probs, k= self.k, dim=-1)      # (T, k) (T, k)\n",
    "\n",
    "        # Calculating argmax \n",
    "        top1_idx = torch.argmax(probs, dim=-1)                           # (T,)\n",
    "        \n",
    "        # Normalise topk values as sum is not equal to 1\n",
    "        topk_prob = topk_prob / (topk_prob.sum(dim=-1, keepdim=True) + 1e-9)\n",
    "\n",
    "        # Dispatch tokens to experts \n",
    "        y_flat = torch.zeros_like(x_flat)\n",
    "        for i in range(self.k):\n",
    "            expert_idx = topk_idx[:, i]                                  # (T,)\n",
    "            expert_prob = topk_prob[:, i]\n",
    "\n",
    "            for e, expert in enumerate(self.experts):\n",
    "                mask = (expert_idx == e)                                 # (T,)\n",
    "                if mask.any():\n",
    "                    tokens_e = x_flat[mask]                              # (n_e, D)\n",
    "                    out_e = expert(tokens_e)                             # (n_e, D)\n",
    "                    # Weighed sum\n",
    "                    y_flat[mask] += expert_prob[mask].unsqueeze(-1) * out_e\n",
    "\n",
    "        y = y_flat.view(B, S, D)\n",
    "\n",
    "        # Aux load balancing \n",
    "        aux_load = self._load_balancing_loss(probs, top1_idx)\n",
    "\n",
    "        return y, aux_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be071886",
   "metadata": {},
   "source": [
    "### Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "B, S, D = 4, 10, 32\n",
    "\n",
    "\n",
    "moe= MoE(d_model=D, d_hidden= 64, num_experts= 4, k= 2, activation=\"relu\", dropout=0.1)\n",
    "\n",
    "x = torch.randn(B,S,D)\n",
    "y, aux_loss= moe(x)\n",
    "\n",
    "print(\"Output shape:\", y.shape)          # (B, S, D)\n",
    "print(\"Aux loss:\", aux_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
